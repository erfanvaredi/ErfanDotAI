{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG + React Agents + LlamaIndex Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq llama_index llama_hub wget pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex,SimpleDirectoryReader\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "from llama_index.tools.query_engine import QueryEngineTool\n",
    "\n",
    "from llama_index.agent import ReActAgent\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Arxiv Papers\n",
    "Install wget in your system.\n",
    "On Mac:\n",
    "```sh\n",
    "brew install wget\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-22 20:41:21--  https://arxiv.org/pdf/2312.04511.pdf\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.195.42, 151.101.3.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 755837 (738K) [application/pdf]\n",
      "Saving to: ‘../files/papers/llm_compiler_2312.04511.pdf’\n",
      "\n",
      "../files/papers/llm 100%[===================>] 738.12K  3.55MB/s    in 0.2s    \n",
      "\n",
      "2024-01-22 20:41:24 (3.55 MB/s) - ‘../files/papers/llm_compiler_2312.04511.pdf’ saved [755837/755837]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2312.04511.pdf\" -O \"../files/papers/llm_compiler_2312.04511.pdf\"\n",
    "# !wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2312.06648.pdf\" -O \"../files/papers/dense_x_retrieval_2312.06648.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate OpenAI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\",temperature=0)\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, Parse, Index and Create Retrival Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erfan/miniconda3/envs/ai-research/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 21/21 [00:00<00:00, 437.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len docs: 21\n",
      "len nodes: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = SimpleDirectoryReader('../files/papers/').load_data()\n",
    "\n",
    "nodes = service_context.node_parser.get_nodes_from_documents(docs, show_progress=True)\n",
    "\n",
    "print(f'len docs: {len(docs)}')\n",
    "print(f'len nodes: {len(nodes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_nodes = VectorStoreIndex(nodes, service_context=service_context)\n",
    "\n",
    "retriver_engine_nodes = index_nodes.as_retriever(similarity_top_k=3)\n",
    "\n",
    "query_engine_nodes = index_nodes.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMCompiler is a novel framework that optimizes the parallel function calling performance of Language Model Models (LLMs). It enables the efficient orchestration of multiple function calls and their dependencies, resulting in improved latency, cost, and accuracy. LLMCompiler consists of three key components: an LLM Planner, a Task Fetching Unit, and an Executor. \n",
      "\n",
      "The LLM Planner identifies the execution flow by defining different function calls and their dependencies based on user inputs. The Task Fetching Unit dispatches the function calls that can be executed in parallel after substituting variables with the actual outputs of preceding tasks. The Executor executes the dispatched function calling tasks using the associated tools.\n",
      "\n",
      "LLMCompiler has several use cases. It can be used with open-source LLMs to empower them with the capability to efficiently handle multiple function calling. It can also be beneficial for GPT models. LLMCompiler has been evaluated on various tasks with different patterns of parallel function calling. It has shown significant improvements in latency, cost reduction, and accuracy compared to existing frameworks.\n",
      "\n",
      "LLMCompiler supports embarrassingly parallel function calling patterns as well as more complex function calling patterns with different types of dependencies between tasks. It also supports function calling patterns that require dynamic replanning, allowing the execution flow to be determined based on intermediate results.\n",
      "\n",
      "To use LLMCompiler, users need to specify the tools that LLMs can use, including their descriptions and argument specifications. Optionally, users can provide in-context examples demonstrating the usage of these tools. Users can also provide examples to guide the behavior of the LLM Planner.\n",
      "\n",
      "Overall, LLMCompiler is a powerful framework that optimizes the parallel function calling performance of LLMs, enabling efficient and accurate execution of multiple function calls with their dependencies.\n",
      "[NodeWithScore(node=TextNode(id_='718df65b-711c-4328-918a-4bd158f5de17', embedding=None, metadata={'page_label': '3', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 755837, 'creation_date': '2024-01-22', 'last_modified_date': '2023-12-08', 'last_accessed_date': '2024-01-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='180f362a-528d-4e42-b46f-992594e2bc22', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '3', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 755837, 'creation_date': '2024-01-22', 'last_modified_date': '2023-12-08', 'last_accessed_date': '2024-01-22'}, hash='f0defd116ceebf0f19be30be2e9c93e03e0c72b2733f8784122a254db57344cb'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='36c7b1cd-6256-491f-b693-2dd6115a98c6', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '2', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 755837, 'creation_date': '2024-01-22', 'last_modified_date': '2023-12-08', 'last_accessed_date': '2024-01-22'}, hash='c5d5918c4b7f74e1a6d583c45bb393e498146249c6dab0ab7cd330e2c2a38634'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='a363e61a-936d-4d4a-b300-1458c2c20a57', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='55c8f74a6d472ef5f2d08ddd84f9972866b9872dbceb0e937fb46d4d86ac2050')}, hash='80346e7cbac29e8a141a0074ecb840a79eb93eca27ea8ee6340df5e261f69664', text='efficiently orchestrate various function calls and handle their dependencies. This shares a similar philosophy to the\\nrecent studies that view LLMs as an operating system [26, 41]. To this end, we introduce LLMCompiler , a novel\\nframework that enables parallel multi-tool execution of LLMs across different models and workloads. To the best of\\nour knowledge, LLMCompiler is the first framework to specifically optimize the orchestration of multi-function call-\\ning of LLMs that can not only improve latency and cost, but also result in higher accuracy by minimizing interference\\nfrom processing intermediate results. In more detail, we make the following contributions:\\n• We introduce LLMCompiler , an LLM compiler that optimizes the parallel function calling performance of LLMs.\\nAt a high level, this is achieved by introducing three key components: (i) an LLM Planner (Sec. 3.1) that identifies\\nan execution flow from user inputs that defines different function calls with their dependencies; (ii) a Task Fetching\\nUnit (Sec. 3.2) that dispatches the function calls which can be executed in parallel after substituting variables with\\nthe actual outputs of the preceding tasks; and (iii) an Executor (Sec. 3.3) that executes the dispatched function\\ncalling tasks using the associated tools. See Figure 2 and Sec. 3 for the detailed system overview.\\n• We extensively evaluate the accuracy and latency (Table 1) as well as token usage (Table 2) of LLMCompiler on\\na range of problems with different types of function calling patterns. LLMCompiler can be used in conjunction\\nwith open-source LLMs, such as LLaMA-2, demonstrating its effectiveness in empowering open-source models\\nwith the capability to efficiently handle multiple function calling. LLMCompiler can also be beneficial for GPT\\nmodels. Our results show latency improvements of up to 1.35×as compared to OpenAI’s parallel function calling\\ncapability which was released concurrently to our work.\\n• We test LLMCompiler on various tasks that exhibit different patterns of parallel function calling:\\n◦We evaluate LLMCompiler onembarrassingly parallel function calling patterns using the HotpotQA [61]\\nand Movie Recommendation [7] benchmarks, which are two-way and eight-way parallelizable, respectively.\\nWe observe 1.80×/3.74×speedup and 3.37×/6.73×cost reduction for each dataset, as compared to ReAct\\n(Sec. 4.1).\\n◦To test the performance on more complex function calling patterns, we have created a new dataset called Paral-\\nlelQA which includes parallel function calls with different types of dependencies between the tasks (Figure 3).\\nThe results given in Table 1 exhibit a similar trend with up to 2.27×speedup and 4.65×cost reduction com-\\npared to ReAct, as well as ∼9%improved accuracy using the LLaMA-2 model (Sec. 4.2).\\n◦We evaluate LLMCompiler ’s capability in supporting function calling patterns that require dynamic replan-\\nning, which is achieved through a feedback loop from the Executor back to our LLM Planner. For the Game\\nof 24 challenge [64], which requires repeated replanning based on the intermediate results, LLMCompiler\\ndemonstrates a 2 ×speedup, compared to Tree-of-Thoughts (Sec. 4.3).\\n2 Related Work\\n2.1 Latency Optimization in LLMs\\nVarious studies have focused on optimizing model design [28, 17, 36, 14, 30, 16, 29, 10, 33] and systems [31, 66, 2, 3]\\nfor efficient LLM inference. Optimizations at the application level, however, are less explored. This is critical from a\\npractical point of view for situations involving black-box LLM models and services where modifications to the models\\nand the underlying inference pipeline are highly restricted.\\nSkeleton-of-Thought [39] recently proposed to reduce latency through application-level parallel decoding. This\\nmethod involves a two-step process of an initial skeleton generation phase, similar to our planning stage, followed by\\nparallel execution of skeleton items. However, it is primarily designed for embarrassingly parallelizable workloads and\\ndoes not support problems that have inherently interdependent tasks, as it operates under the assumption of having no\\ndependencies between skeleton tasks. This limits its applicability in complex scenarios such as coding [11, 38, 20, 6]\\nor math [22, 21] problems, as also stated in their paper [39].', start_char_idx=0, end_char_idx=4285, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8592233790276087), NodeWithScore(node=TextNode(id_='d1900e70-3bad-4aa6-9f3d-fbf1e9ed00aa', embedding=None, metadata={'page_label': '4', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 755837, 'creation_date': '2024-01-22', 'last_modified_date': '2023-12-08', 'last_accessed_date': '2024-01-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='613ef7ea-6c9a-4f03-a557-6b941f596f8f', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '4', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 755837, 'creation_date': '2024-01-22', 'last_modified_date': '2023-12-08', 'last_accessed_date': '2024-01-22'}, hash='d511600a0096844294359445847b4bbf3569ad1d412f8880e0abfdcc551f39db'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a363e61a-936d-4d4a-b300-1458c2c20a57', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '3', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 755837, 'creation_date': '2024-01-22', 'last_modified_date': '2023-12-08', 'last_accessed_date': '2024-01-22'}, hash='55c8f74a6d472ef5f2d08ddd84f9972866b9872dbceb0e937fb46d4d86ac2050'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='b53d657f-0b6d-4e7b-92a5-4ba3c1c79839', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='634664c12b6131b06f3276099181e0f0f8340d9ae5476fdd2648bd9241e4c9a7')}, hash='6a7d5e2d7241786a38561087266c21a68b13fb48ccd38e195f7fc43244014fb4', text='68, 5, 13, 9], with different capabilities and applications, as well as parameter-efficient training techniques [32, 24, 23]\\nfor finetuning and customizing. LLMCompiler enables efficient parallel function calling for open-source models,\\nand also, as we will show later in Sec. 4, it can potentially achieve better latency as compared to OpenAI’s recent\\nparallel function calling capability.\\n2.2 Tool-Augmented LLMs\\nEarly applications of LLMs were limited to language understanding and generation tasks. However, the reasoning\\ncapability of LLMs enabled exploring LLMs as independent agents, each of which can carry a different task. This\\nconcept was then expanded to function calling capability where the LLM could even specify the function’s arguments.\\nNotable works here include the early work of [47], which produced a custom LLM output format that would be\\nparsed to perform LLM information retrieval. The key here was that the LLM decided what the inputs for calling\\nthe functions should be, as well as where to insert the result. Subsequent work [35, 48] developed frameworks for\\ncalling many different external APIs to support allowing these agents to execute complex tasks. ReAct [65] presented\\na study where LLMs interact with external environments using an API, integrating reasoning and action generation for\\nimproved performance in various tasks such as QA, fact verification, and decision-making benchmarks [61, 52, 49, 63].\\nGorilla [43] is a finetuned LLM designed to translate natural language input to function calls given documentation for\\nthe APIs, aiming to allow LLMs to retrieve and update documentation when they change. In an effort to simulate and\\nevaluate on real-world API calls, Toolbench [45] and API-Bank [34] introduced benchmarks and datasets for tool-\\naugmented LLMs in different domains. RestGPT [50] extended LLMs to support REST APIs, which allows them to\\ninteract with web services and clients. Moreover, OpenAI [40] released their own function calling capabilities within\\ntheir API, allowing their LLMs to return formatted JSON for execution.\\n2.3 Plan and Solve Strategy\\nSeveral studies [27, 59, 69, 42, 44, 70, 19, 55] have explored prompting methods of breaking down complex queries\\ninto various levels of detail to solve them, thereby improving LLM’s performance in reasoning tasks. Specifically,\\nDecomposed Prompting [27] is the method to tackle complex tasks by decomposing them into simpler sub-tasks, each\\noptimized through dedicated prompting-based LLMs. Step-Back Prompting [69] presented the LLM prompt allowing\\nLLMs to abstract high-level concepts from details, significantly enhancing reasoning abilities across various tasks.\\nAdding to this, Plan-and-Solve Prompting [55] segments multi-step reasoning tasks into subtasks to minimize errors\\nand improve task accuracy without manual prompting. However, note that these methods primarily focus on improving\\nthe accuracy on reasoning benchmarks. In contrast, LLMCompiler uses a planner to identify parallelizable patterns\\nwithin queries, aiming to reduce latency while maintaining accuracy.\\nA notable work is ReWOO [60] which employs a planner to separate the reasoning process from the execution and\\nobservation phases, aiming to decrease token usage and cost as compared to ReAct. Our approach is different from\\nReWOO in multiple aspects. First, LLMCompiler allows parallel function calling which can reduce latency as well\\nas cost. Second, LLMCompiler supports dynamic replanning which is important for problems whose execution flow\\ncannot be determined statically in the beginning (Sec. 4.3).\\n3 Methodology\\nTo illustrate the system components of LLMCompiler , we begin with a simple 2-way parallel example shown in Fig-\\nure 2. Consider the user question, “How much does Microsoft’s market cap need to increase to exceed Apple’s market\\ncap?” To answer this question, the LLM first needs to perform web searches to find the market cap of Apple and\\nMicrosoft, and then divide Apple’s market cap by that of Microsoft. While the existing frameworks, including ReAct,\\nperform these tasks in a sequential, one-at-a-time manner, it is evident that the two search queries can be executed in\\nparallel. The important question is how to automatically detect which tasks can be performed in parallel and which\\nones are interdependent, and then to orchestrate the execution of the different tasks accordingly.', start_char_idx=0, end_char_idx=4396, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8502768170710224), NodeWithScore(node=TextNode(id_='1edc9d31-5c33-4fbc-94da-30ca07fc17bd', embedding=None, metadata={'page_label': '6', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 755837, 'creation_date': '2024-01-22', 'last_modified_date': '2023-12-08', 'last_accessed_date': '2024-01-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3bf35968-f4d0-4291-9abd-26e9fb3963f4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 755837, 'creation_date': '2024-01-22', 'last_modified_date': '2023-12-08', 'last_accessed_date': '2024-01-22'}, hash='93ee7350e138772ea4dae56f2d1e34ff14b213289eb7b844b5542d4c11ee92b0'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='adcdb433-ee03-421b-879a-827598bc305b', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '5', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 755837, 'creation_date': '2024-01-22', 'last_modified_date': '2023-12-08', 'last_accessed_date': '2024-01-22'}, hash='848d4f562a988225b39ea0108385d933814790471b8dbaf8abca30be30677392'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='4d342b45-eb0f-4d12-b739-dde66c3398f9', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='317237e52bfbdbb07b70df32110ca653187e894cb44087c74ebe3255d8c4e521')}, hash='93ee7350e138772ea4dae56f2d1e34ff14b213289eb7b844b5542d4c11ee92b0', text='3.3 Executor\\nThe Executor asynchronously executes tasks fetched from the Task Fetching Unit. As the Task Fetching Unit guar-\\nantees all the tasks dispatched to the Executor are independent, the Executor can simply execute them concurrently.\\nThe Executor is equipped with the tools that the user provides, and it delegates the task to the associated tool. These\\ntools can be simple functions like a calculator, Wikipedia search, or API calls, or they can even be LLM agents that are\\ntailored for a specific task. For a practical application example, consider a scenario that involves gathering information\\nfrom various sources. An example could involve summarizing interactions with a particular company by retrieving\\ndata from sales records, email communications, and meeting transcripts. Each of these tasks can be handled in parallel\\nusing an LLM, whose results can be subsequently combined through a join operation to produce a final summary.\\nAs depicted in the Executor block of Figure 2, each task has dedicated memory to store its intermediate outcomes,\\nsimilar to what typical sequential frameworks do when aggregating observations as a single prompt [65]. Upon\\ncompletion of the task, the final results (i.e., observations) are forwarded as input to the tasks dependent on them.\\n3.4LLMCompiler Details\\n3.4.1 Streamed Planner\\nThe Planner may incur a non-trivial overhead for user queries that involve a lot of tasks to be executed. This is because\\nthe Planner operates as a blocking call since both the Task Fetching Unit and the Executor must wait for its output\\nbefore initiating their processes. However, this can be mitigated by enabling the Planner to asynchronously stream the\\ndependency graph, analogous to instruction pipelining in modern computer systems. This method allows each task to\\nbe immediately processed by the Task Fetching Unit and forwarded to the Executor as soon as all of its dependencies\\nare resolved. In Appendix. A.2.2, we show that this streaming approach can result in up to 30% improved latency.\\n3.4.2 Dynamic Replanning\\nIn various applications, the execution graph may need to adapt based on intermediate results that are a priori unknown.\\nA similar analogy in programming is branching, where the path of execution is determined only during runtime,\\ndepending on which branch conditions are satisfied. Such dynamic execution patterns can also appear with LLM\\nfunction calling. For simple branching (e.g., if-else statements) one could statically compile the execution flow and\\nchoose the right dynamically based on the intermediate results. However, for more complex branching it may be better\\nto do a recompilation or replanning based on the intermediate results. We show an example use case of this in Sec. 4.3\\nfor solving the Game of 24 using the Tree-of-Thoughts approach.\\nIn replanning, the Executor sends the intermediate results back to our LLM Planner. Based on that, the Planner\\nproduces a new set of tasks with their associated dependencies and dispatches them to the Task Fetching Unit and then\\nthe Executor. This process is repeated until the final result is achieved.\\n3.5 User-Supplied Information\\nThe details of Figure 2 are abstracted away from the user as LLMCompiler can automatically identify and execute\\nthe optimal parallel execution flow. From the user’s perspective, only the following is required to use LLMCompiler :\\n1.Tool Definitions : Users need to specify the tools that LLMs can use, including their descriptions and argument\\nspecifications. Optionally, users can also provide in-context examples demonstrating the usage of these tools. This\\nis essentially the same requirement as other frameworks like ReAct and OpenAI function calling [65, 4].\\n2.In-context Examples for the Planner : Optionally, users can provide LLMCompiler with examples of how the\\nPlanner should behave. For instance, in the case of Figure 2, users may provide examples illustrating expected\\ninter-task dependencies for certain queries. Such examples can aid the Planner LLM in generating the appropriate\\ndependency graph in the correct format for incoming inputs. In Appendix A.3, we include the examples that we\\nused in our evaluation (Sec. 4).\\n6', start_char_idx=0, end_char_idx=4187, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8497174583313575)]\n",
      "{'718df65b-711c-4328-918a-4bd158f5de17': {'page_label': '3', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 755837, 'creation_date': '2024-01-22', 'last_modified_date': '2023-12-08', 'last_accessed_date': '2024-01-22'}, 'd1900e70-3bad-4aa6-9f3d-fbf1e9ed00aa': {'page_label': '4', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 755837, 'creation_date': '2024-01-22', 'last_modified_date': '2023-12-08', 'last_accessed_date': '2024-01-22'}, '1edc9d31-5c33-4fbc-94da-30ca07fc17bd': {'page_label': '6', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 755837, 'creation_date': '2024-01-22', 'last_modified_date': '2023-12-08', 'last_accessed_date': '2024-01-22'}}\n"
     ]
    }
   ],
   "source": [
    "resp = query_engine_nodes.query(\"Explain LLMCompiler and its usecases.\")\n",
    "\n",
    "print(resp.response)\n",
    "print(resp.source_nodes)\n",
    "print(resp.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node-0 has the score of 0.8592233790276087\n",
      "Node-1 has the score of 0.8502768170710224\n",
      "Node-2 has the score of 0.8497174583313575\n"
     ]
    }
   ],
   "source": [
    "for idx, node_with_score in enumerate(resp.source_nodes):\n",
    "    print(f'Node-{idx} has the score of {node_with_score.score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tool and ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tool = QueryEngineTool.from_defaults(\n",
    "    name='llmcompiler',\n",
    "    query_engine=query_engine_nodes,\n",
    "    description=(\n",
    "        \"Provides information about LLMCompiler and Parallel Function Calling.\"\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "react_agent = ReActAgent.from_tools(\n",
    "    [query_engine_tool],\n",
    "    max_function_calls=10,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Ask From Our Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: I need to use a tool to help me answer the question.\n",
      "Action: llmcompiler\n",
      "Action Input: {'input': 'Parallel Function Calling'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Parallel function calling refers to the ability of Large Language Models (LLMs) to execute multiple function calls simultaneously. This allows LLMs to efficiently handle complex tasks by invoking different functions and coordinating their execution. The goal of parallel function calling is to reduce latency, cost, and improve accuracy by executing function calls in parallel rather than sequentially. LLMCompiler is a framework that optimizes the orchestration of parallel function calling in LLMs by introducing an LLM Planner, a Task Fetching Unit, and an Executor. This framework streamlines the execution of multiple function calls and handles their dependencies, resulting in improved performance and efficiency.\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I can answer without using any more tools.\n",
      "Answer: Parallel function calling refers to the ability of Large Language Models (LLMs) to execute multiple function calls simultaneously. This allows LLMs to efficiently handle complex tasks by invoking different functions and coordinating their execution. LLMCompiler is a framework that optimizes the orchestration of parallel function calling in LLMs, improving performance and efficiency by introducing an LLM Planner, a Task Fetching Unit, and an Executor.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "react_agent_resp = react_agent.chat(\"What is Parallel Function Calling and How LLMCompilers can help?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE: Parallel function calling refers to the ability of Large Language Models (LLMs) to execute multiple function calls simultaneously. This allows LLMs to efficiently handle complex tasks by invoking different functions and coordinating their execution. LLMCompiler is a framework that optimizes the orchestration of parallel function calling in LLMs, improving performance and efficiency by introducing an LLM Planner, a Task Fetching Unit, and an Executor.\n",
      "OBJECT_KEYS: dict_keys(['response', 'sources', 'source_nodes'])\n",
      "SOURCE_NODES: [NodeWithScore(node=TextNode(id_='6b1611f0-a599-4f88-8673-03f5e818a163', embedding=None, metadata={'page_label': '1', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 755837, 'creation_date': '2024-01-22', 'last_modified_date': '2023-12-08', 'last_accessed_date': '2024-01-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c9803b66-8538-4b25-831f-1fe396e0f1e7', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 755837, 'creation_date': '2024-01-22', 'last_modified_date': '2023-12-08', 'last_accessed_date': '2024-01-22'}, hash='9f10c274865e1d57ddb08c38e1457d38984cd6c916f5682249a264dfe7582d93'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2f92f70f-6e31-4275-bcc6-1c6d5bf1167c', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': '../files/papers/i.py', 'file_name': 'i.py', 'file_type': 'text/x-python', 'file_size': 0, 'creation_date': '2024-01-22', 'last_modified_date': '2024-01-22', 'last_accessed_date': '2024-01-22'}, hash='44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='baa33ca2-69ef-4993-8837-eb80365578a3', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='712115b28f3a4789a9902a978a4f5a1b8d902f28c7ce5d866f8824a24024860e')}, hash='9f10c274865e1d57ddb08c38e1457d38984cd6c916f5682249a264dfe7582d93', text='An LLM Compiler for Parallel Function Calling\\nSehoon Kim *1Suhong Moon∗1Ryan Tabrizi1Nicholas Lee1\\nMichael W. Mahoney1,2,3Kurt Keutzer1Amir Gholami1,2\\n1UC Berkeley2ICSI3LBNL\\n{sehoonkim, suhong.moon, rtabrizi, nicholas lee, mahoneymw, keutzer, amirgh }@berkeley.edu\\nAbstract\\nLarge Language Models (LLMs) have shown remarkable results on various complex reasoning benchmarks. The\\nreasoning capabilities of LLMs enable them to execute function calls, using user-provided functions to overcome\\ntheir inherent limitations, such as knowledge cutoffs, poor arithmetic skills, or lack of access to private data. This\\ndevelopment has expanded LLMs’ scope to include multi-function calling, where LLMs are equipped with a variety\\nof functions and select the proper functions based on the context. Multi-function calling abilities of LLMs have\\ncatalyzed LLM-based software development, allowing them to tackle more complex problems. However, current\\nmethods for multi-function calling often require sequential reasoning and acting for each function which can result\\nin high latency, cost, and sometimes inaccurate behavior. To address this, we introduce LLMCompiler , which\\nexecutes functions in parallel to efficiently orchestrate multi-function calling. Drawing from the principles of classical\\ncompilers, LLMCompiler streamlines parallel function calling in LLMs with three components: (i) an LLM Planner,\\nformulating execution strategies and dependencies; (ii) a Task Fetching Unit, dispatching and updating function\\ncalling tasks; and (iii) an Executor, executing these tasks in parallel. With LLMCompiler , the user specifies the tools\\nalong with optional in-context examples, and LLMCompiler automatically computes an optimized orchestration for\\nthe function calls. Importantly, LLMCompiler can be used with open-source models such as LLaMA-2, as well\\nas OpenAI’s GPT models. We have benchmarked LLMCompiler on a range of tasks that exhibit different types of\\nparallel function calling, including cases with non-trivial inter-dependency between function calls, as well as cases\\nthat require dynamic replanning based on intermediate results. We observe consistent latency speedup of up to 3.7×,\\ncost savings of up to 6.7×, and accuracy improvement of up to ∼9%as compared to ReAct. Additionally, due to\\nour more efficient orchestration of multiple function calls, LLMCompiler achieves up to 1.35×latency gain over\\nOpenAI’s recent parallel function calling feature, while achieving similar accuracy. Our code has been open-sourced.1\\n1 Introduction\\nRecent advancements in the reasoning capability of Large Language Models (LLMs) have expanded the applicability\\nof LLMs beyond content generation to solving complex problems [57, 58, 64, 8, 56, 70, 12, 62, 18]; and recent works\\nhave shown how this reasoning capability could be helpful in improving accuracy for solving complex and logical tasks\\nsuch as the Game of 24.2The reasoning capability has also allowed function (also known as tool) calling capability.\\nHere, the LLM can invoke the provided functions and use the function outputs to help it complete its task. These\\nfunctions range from a simple calculator that the LLM can invoke for arithmetic operations to complex functions\\nusing LLMs to return their results. A practical use case would be a function that queries a knowledge source and\\nsummarizes the data before returning the results back to the LLM.\\nThe ability of LLMs to integrate various tools and coordinate multiple function calls could enable a fundamental\\nshift in how we develop LLM-based software. However, this brings up an important challenge: what is the most\\neffective approach to incorporate multiple function calls? A notable approach has been introduced in ReAct [65],\\n*Equal contribution\\n1https://github.com/SqueezeAILab/LLMCompiler\\n2In this problem, the LLM is given four numbers, and it is tasked with finding a set of math operations between them that end up with 24 as\\nthe final result. For this problem, even GPT-4 has a zero-shot accuracy of only 4%, whereas using Tree-of-Thoughts reasoning can increase the\\naccuracy beyond 70% [64].\\n1arXiv:2312.04511v1  [cs.CL]  7 Dec 2023', start_char_idx=0, end_char_idx=4156, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8376943949678676), NodeWithScore(node=TextNode(id_='718df65b-711c-4328-918a-4bd158f5de17', embedding=None, metadata={'page_label': '3', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 755837, 'creation_date': '2024-01-22', 'last_modified_date': '2023-12-08', 'last_accessed_date': '2024-01-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='180f362a-528d-4e42-b46f-992594e2bc22', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '3', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 755837, 'creation_date': '2024-01-22', 'last_modified_date': '2023-12-08', 'last_accessed_date': '2024-01-22'}, hash='f0defd116ceebf0f19be30be2e9c93e03e0c72b2733f8784122a254db57344cb'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='36c7b1cd-6256-491f-b693-2dd6115a98c6', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '2', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 755837, 'creation_date': '2024-01-22', 'last_modified_date': '2023-12-08', 'last_accessed_date': '2024-01-22'}, hash='c5d5918c4b7f74e1a6d583c45bb393e498146249c6dab0ab7cd330e2c2a38634'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='a363e61a-936d-4d4a-b300-1458c2c20a57', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='55c8f74a6d472ef5f2d08ddd84f9972866b9872dbceb0e937fb46d4d86ac2050')}, hash='80346e7cbac29e8a141a0074ecb840a79eb93eca27ea8ee6340df5e261f69664', text='efficiently orchestrate various function calls and handle their dependencies. This shares a similar philosophy to the\\nrecent studies that view LLMs as an operating system [26, 41]. To this end, we introduce LLMCompiler , a novel\\nframework that enables parallel multi-tool execution of LLMs across different models and workloads. To the best of\\nour knowledge, LLMCompiler is the first framework to specifically optimize the orchestration of multi-function call-\\ning of LLMs that can not only improve latency and cost, but also result in higher accuracy by minimizing interference\\nfrom processing intermediate results. In more detail, we make the following contributions:\\n• We introduce LLMCompiler , an LLM compiler that optimizes the parallel function calling performance of LLMs.\\nAt a high level, this is achieved by introducing three key components: (i) an LLM Planner (Sec. 3.1) that identifies\\nan execution flow from user inputs that defines different function calls with their dependencies; (ii) a Task Fetching\\nUnit (Sec. 3.2) that dispatches the function calls which can be executed in parallel after substituting variables with\\nthe actual outputs of the preceding tasks; and (iii) an Executor (Sec. 3.3) that executes the dispatched function\\ncalling tasks using the associated tools. See Figure 2 and Sec. 3 for the detailed system overview.\\n• We extensively evaluate the accuracy and latency (Table 1) as well as token usage (Table 2) of LLMCompiler on\\na range of problems with different types of function calling patterns. LLMCompiler can be used in conjunction\\nwith open-source LLMs, such as LLaMA-2, demonstrating its effectiveness in empowering open-source models\\nwith the capability to efficiently handle multiple function calling. LLMCompiler can also be beneficial for GPT\\nmodels. Our results show latency improvements of up to 1.35×as compared to OpenAI’s parallel function calling\\ncapability which was released concurrently to our work.\\n• We test LLMCompiler on various tasks that exhibit different patterns of parallel function calling:\\n◦We evaluate LLMCompiler onembarrassingly parallel function calling patterns using the HotpotQA [61]\\nand Movie Recommendation [7] benchmarks, which are two-way and eight-way parallelizable, respectively.\\nWe observe 1.80×/3.74×speedup and 3.37×/6.73×cost reduction for each dataset, as compared to ReAct\\n(Sec. 4.1).\\n◦To test the performance on more complex function calling patterns, we have created a new dataset called Paral-\\nlelQA which includes parallel function calls with different types of dependencies between the tasks (Figure 3).\\nThe results given in Table 1 exhibit a similar trend with up to 2.27×speedup and 4.65×cost reduction com-\\npared to ReAct, as well as ∼9%improved accuracy using the LLaMA-2 model (Sec. 4.2).\\n◦We evaluate LLMCompiler ’s capability in supporting function calling patterns that require dynamic replan-\\nning, which is achieved through a feedback loop from the Executor back to our LLM Planner. For the Game\\nof 24 challenge [64], which requires repeated replanning based on the intermediate results, LLMCompiler\\ndemonstrates a 2 ×speedup, compared to Tree-of-Thoughts (Sec. 4.3).\\n2 Related Work\\n2.1 Latency Optimization in LLMs\\nVarious studies have focused on optimizing model design [28, 17, 36, 14, 30, 16, 29, 10, 33] and systems [31, 66, 2, 3]\\nfor efficient LLM inference. Optimizations at the application level, however, are less explored. This is critical from a\\npractical point of view for situations involving black-box LLM models and services where modifications to the models\\nand the underlying inference pipeline are highly restricted.\\nSkeleton-of-Thought [39] recently proposed to reduce latency through application-level parallel decoding. This\\nmethod involves a two-step process of an initial skeleton generation phase, similar to our planning stage, followed by\\nparallel execution of skeleton items. However, it is primarily designed for embarrassingly parallelizable workloads and\\ndoes not support problems that have inherently interdependent tasks, as it operates under the assumption of having no\\ndependencies between skeleton tasks. This limits its applicability in complex scenarios such as coding [11, 38, 20, 6]\\nor math [22, 21] problems, as also stated in their paper [39].', start_char_idx=0, end_char_idx=4285, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8155120119688462), NodeWithScore(node=TextNode(id_='f64b8b55-e27a-4004-baf1-8f0f92d602b5', embedding=None, metadata={'page_label': '8', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 755837, 'creation_date': '2024-01-22', 'last_modified_date': '2023-12-08', 'last_accessed_date': '2024-01-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='90802793-67c3-4c5f-a5d9-168b8f54b208', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '8', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 755837, 'creation_date': '2024-01-22', 'last_modified_date': '2023-12-08', 'last_accessed_date': '2024-01-22'}, hash='8bda38f865e02fb4a106e3865fb87fd7c99c92541b3ab944a9d388225e687deb'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1c9331a0-e1de-4a84-bddc-e6100c811ea8', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '8', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 755837, 'creation_date': '2024-01-22', 'last_modified_date': '2023-12-08', 'last_accessed_date': '2024-01-22'}, hash='654596197192731752a81903c9e4948050917d3d4c516224c10cf545df6f4828'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='22c25e34-5564-4547-b7ef-679aca3ff813', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='d807a02c2acf85e88991637e4e44fc40de44026db5d5f3ca7c134b44e0b5c5f6')}, hash='2703edfc83c60e6c2990361a46faccad7aa180857b8b27da554bab53e4f2c716', text='Costs. Another important consideration when using LLMs is cost, which depends on the input and output token\\nusage. The costs for GPT experiments are provided in Table 2. LLMCompiler is more efficient than ReAct with\\nrespect to cost, which requires frequent LLM invocations for each tool usage, as illustrated in Figure 1. Interestingly,\\nLLMCompiler also outperforms the recent OpenAI parallel function calling in cost efficiency, for two main reasons.\\nFirst, LLMCompiler ’s planning phase is more efficient in prompt length since our Planner’s in-context examples\\nonly need to include plans, not observations. Second, LLMCompiler detects the dependency pattern only in the\\nbeginning, removing repetitive LLM calls used to identify new dependencies upon completing ongoing function calls.\\nThis becomes particularly evident in workloads with more complex dependency patterns, such as those in Figure 3\\n(b) and (c). For instance, in Figure 3 (c), OpenAI’s parallel function calling would require separate LLM calls to\\nidentify the initial search tasks, subsequent math tasks, and then the final math task, since it can only output a set of\\nparallelizable tasks at each step. We explore parallel function calling with more complex dependencies in more detail\\nin Sec. 4.2.\\n4.2 Parallel Function Calling with Dependencies\\nThe cases considered above are rather simple, as only one tool is used and all tasks can be executed independently\\nof one another. However, similar to code execution in traditional code blocks, we may encounter function calling\\nscenarios that involve more complex dependencies. To systematically evaluate the capability to plan out function\\ncalling in scenarios that involve complex task dependencies, we have designed a custom benchmark called ParallelQA.\\nThis benchmark is designed to incorporate non-trivial function calling patterns, including three different types of\\n8', start_char_idx=3427, end_char_idx=5317, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8143455106842639)]\n"
     ]
    }
   ],
   "source": [
    "print(f'RESPONSE: {react_agent_resp.response}')\n",
    "print(f'OBJECT_KEYS: {react_agent_resp.__dict__.keys()}')\n",
    "print(f'SOURCE_NODES: {react_agent_resp.source_nodes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len SOURCE_NODES: 3\n"
     ]
    }
   ],
   "source": [
    "print(f'len SOURCE_NODES: {len(react_agent_resp.source_nodes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMemoryBuffer(token_limit=3072, tokenizer_fn=functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all'), chat_store=SimpleChatStore(store={'chat_history': [ChatMessage(role=<MessageRole.USER: 'user'>, content='What is Parallel Function Calling and How LLMCompilers can help?', additional_kwargs={}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Parallel function calling refers to the ability of Large Language Models (LLMs) to execute multiple function calls simultaneously. This allows LLMs to efficiently handle complex tasks by invoking different functions and coordinating their execution. LLMCompiler is a framework that optimizes the orchestration of parallel function calling in LLMs, improving performance and efficiency by introducing an LLM Planner, a Task Fetching Unit, and an Executor.', additional_kwargs={})]}), chat_store_key='chat_history')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "react_agent.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content='What is Parallel Function Calling and How LLMCompilers can help?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Parallel function calling refers to the ability of Large Language Models (LLMs) to execute multiple function calls simultaneously. This allows LLMs to efficiently handle complex tasks by invoking different functions and coordinating their execution. LLMCompiler is a framework that optimizes the orchestration of parallel function calling in LLMs, improving performance and efficiency by introducing an LLM Planner, a Task Fetching Unit, and an Executor.', additional_kwargs={})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "react_agent.memory.chat_store.store['chat_history']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
