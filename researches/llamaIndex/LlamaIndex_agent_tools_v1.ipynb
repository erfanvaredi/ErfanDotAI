{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG + React Agents + LlamaIndex Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq llama_index llama-index-core llama-index-llms-openai llama_index llama_hub wget pypdf llama-index-agent-openai\n",
    "\n",
    "# pip install llama-index-llms-replicate\n",
    "# pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Arxiv Papers\n",
    "Install wget in your system.\n",
    "On Mac:\n",
    "```sh\n",
    "brew install wget\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-18 16:09:03--  https://arxiv.org/pdf/2312.04511.pdf\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.3.42, 151.101.131.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 885090 (864K) [application/pdf]\n",
      "Saving to: '../files/papers/llm_compiler_2312.04511.pdf'\n",
      "\n",
      "../files/papers/llm 100%[===================>] 864.35K  1.38MB/s    in 0.6s    \n",
      "\n",
      "2024-02-18 16:09:04 (1.38 MB/s) - '../files/papers/llm_compiler_2312.04511.pdf' saved [885090/885090]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2312.04511.pdf\" -O \"../files/papers/llm_compiler_2312.04511.pdf\"\n",
    "# !wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2312.06648.pdf\" -O \"../files/papers/dense_x_retrieval_2312.06648.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate OpenAI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\",temperature=0)\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, Parse, Index and Create Retrival Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erfan/miniconda3/envs/ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 24/24 [00:00<00:00, 593.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len docs: 24\n",
      "len nodes: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = SimpleDirectoryReader('../files/papers/').load_data()\n",
    "\n",
    "nodes = Settings.node_parser.get_nodes_from_documents(docs, show_progress=True)\n",
    "\n",
    "print(f'len docs: {len(docs)}')\n",
    "print(f'len nodes: {len(nodes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_nodes = VectorStoreIndex(nodes)\n",
    "\n",
    "retriver_engine_nodes = index_nodes.as_retriever(similarity_top_k=3)\n",
    "\n",
    "query_engine_nodes = index_nodes.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMCompiler is a tool that focuses on optimizing Latent Language Models (LLMs) by enabling dynamic replanning and efficient exploration of decision-making environments. It allows for parallel function calling, which can reduce latency and cost, and supports tasks with interdependencies. LLMCompiler is particularly useful for scenarios involving black-box LLM models and services where modifications are restricted. Its capabilities have been demonstrated through experiments, showing significant speedups and improved success rates compared to baselines in tasks like the Game of 24 and WebShop. Additionally, LLMCompiler uses a planner to identify parallelizable patterns within queries, aiming to reduce latency while maintaining accuracy.\n",
      "[NodeWithScore(node=TextNode(id_='5e7a4fe5-faa2-4e15-8e59-0624f4dd0728', embedding=None, metadata={'page_label': '3', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='25035116-56a8-48a5-b983-aadf47c03175', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '3', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, hash='972de646988b3923513c1654c4d04fbb1ba1d0d7f9e0704b71321e4a20d285ae'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e42a6f20-1554-45c1-ad7c-f5071ed50ea4', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '2', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, hash='7c509b808f481df38b8b04478e63d7c6f132f5538326081dbe1c7ad325b54843'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='9c9b7155-30b9-40af-a071-7e2d557a538f', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='2c212ec17504a612ac3a21c424a157c3a73ecb5142acbf1dcdb3a991cdf6b501')}, text='• We evaluate LLMCompiler ’s capability in dynamic replanning, which is achieved through a feedback loop from\\nthe Executor back to our LLM Planner. For the Game of 24 [57], which requires repeated replanning based on the\\nintermediate results, LLMCompiler demonstrates a 2 ×speedup compared to Tree-of-Thoughts (Sec. 4.3).\\n• We showcase that LLMCompiler can explore the interactive decision-making environment effectively and effi-\\nciently. On WebShop, LLMCompiler achieves up to 101.7×speedup and 25.7% improved success rate compared\\nto the baselines. (Sec. 4.4)\\n2 Related Work\\n2.1 Latency Optimization in LLMs\\nVarious studies have focused on optimizing model design [21, 11, 30, 9, 24, 10, 22, 6, 28] and systems [25, 59, 1, 2]\\nfor efficient LLM inference. Optimizations at the application level, however, are less explored. This is critical from a\\npractical point of view for situations involving black-box LLM models and services where modifications to the models\\nand the underlying inference pipeline are highly restricted.\\nSkeleton-of-Thought [34] recently proposed to reduce latency through application-level parallel decoding. This\\nmethod involves a two-step process of an initial skeleton generation phase, followed by parallel execution of skeleton\\nitems. However, it is primarily designed for embarrassingly parallel workloads and does not support problems that\\nhave inherently interdependent tasks, as it assumes no dependencies between skeleton tasks. This limits its applica-\\nbility in complex scenarios such as coding [7, 33, 14, 3] or math [16, 15] problems, as also stated in the paper [34].\\nLLMCompiler addresses this by translating an input query into a series of tasks with inter-dependencies, thereby\\nexpanding the spectrum of problems it can handle.\\nConcurrently to our work, OpenAI has recently introduced a parallel function calling feature in their 1106 re-\\nlease, enhancing user query processing through the simultaneous generation of multiple function calls [36]. Despite\\nits potential for reducing LLM execution time, this feature has certain limitations as it is exclusively available for\\nOpenAI’s proprietary models. However, there is a growing demand for using open-source models driven by the in-\\ncreasing number of open-source LLMs as well as parameter-efficient training techniques [27, 18, 17] for finetuning\\nand customization. LLMCompiler enables efficient parallel function calling for open-source models, and also, as we\\nwill show later in Sec. 4, it can potentially achieve better latency and cost.\\n2.2 Plan and Solve Strategy\\nSeveral studies [52, 38, 40, 63, 13] have explored prompting methods of breaking down complex queries into various\\nlevels of detail to solve them, thereby improving LLM’s performance in reasoning tasks. Specifically, Decomposed\\nPrompting [20] tackles complex tasks by decomposing them into simpler sub-tasks, each optimized through LLMs\\nwith dedicated prompts. Step-Back Prompting [61] enables LLMs to abstract high-level concepts from details to\\nenhance reasoning abilities across various tasks. Plan-and-Solve Prompting [49] segments multi-step reasoning tasks\\ninto subtasks to minimize errors and improve task accuracy without manual prompting. However, these methods\\nprimarily focus on improving the accuracy of reasoning benchmarks. In contrast, LLMCompiler uses a planner to\\nidentify parallelizable patterns within queries, aiming to reduce latency while maintaining accuracy.\\nA notable work is ReWOO [53] which employs a planner to separate the reasoning process from the execution and\\nobservation phases to decrease token usage and cost as compared to ReAct. Our approach is different from ReWOO\\nin multiple aspects. First, LLMCompiler allows parallel function calling which can reduce latency as well as cost.\\nSecond, LLMCompiler supports dynamic replanning which is important for problems whose execution flow cannot\\nbe determined statically in the beginning (Sec. 4.3).\\n2.3 Tool-Augmented LLMs\\nA notable work is Toolformer [43], which produces a custom LLM output to let the LLM decide what the inputs\\nfor calling the functions should be and where to insert the result. This approach has inspired various tool calling\\nframeworks [29, 44]. ReAct [58] proposed to have LLMs interact with external environments through reasoning and\\naction generation for improved performance.', start_char_idx=0, end_char_idx=4349, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8525170579800175), NodeWithScore(node=TextNode(id_='46d166ba-ebc1-4a16-8aaa-fdc8a0770341', embedding=None, metadata={'page_label': '18', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='25cec6b2-a263-4471-bc6c-11fceaf2b188', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '18', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, hash='a9f08efdbd4268a58eca9eba1bfa74a5f30e7ae6584fac6b38f23018bb52c454'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a650ce5e-7c45-49b9-8819-f5e969273938', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '17', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, hash='2b93383551e08e76d773bd9be962b02dc861f52d55fcc5a610df67ee7db466a2'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='64fff4c1-05e7-4a9b-86f6-cfb1981de755', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='7c9705b8c7e689e720fc7caf3106ab3bc17512c31ed985a9390a903366577cf6')}, text='A.3.2 User-Supplied Information\\nLLMCompiler requires the following two inputs from the user:\\n1.Tool Definitions : Users need to specify the tools that LLMs can use, including their descriptions and argument\\nspecifications. Optionally, users can also provide in-context examples demonstrating the usage of these tools. This\\nis essentially the same requirement as other frameworks like ReAct and OpenAI function calling.\\n2.In-context Examples for the Planner : Optionally, users can provide LLMCompiler with examples of how the\\nPlanner should behave. For instance, in the case of Figure 2, users may provide examples illustrating expected\\ninter-task dependencies for certain queries. Such examples can aid the Planner LLM in generating the appropriate\\ndependency graph in the correct format for incoming inputs. In Appendix A.6, we include the examples that we\\nused in our evaluations.\\nA.4 Experiment Details\\nOur experiments evaluate two different common scenarios: (1) using API-based closed-source models; and (2) using\\nopen-source models with an in-house serving framework. We use OpenAI’s GPT models as closed-source models,\\nin particular, gpt-3.5-turbo (1106 release) for HotpotQA and Movie Recommendation, gpt-4-turbo (1106 release) for\\nParallelQA, and gpt-4 (0613 release) for Game of 24. Experiments on HotpotQA, Movie Recommendation, and\\nParallelQA are all conducted in November 2023 after the 1106 release. The Game of 24 experiments are conducted\\nover a two-month period from September to October 2023. For an open-source model, we use LLaMA-2 [48], which\\nwas hosted on 2 A100-80GB GPUs using the vLLM [25] framework. All the runs have been carried out with zero\\ntemperature, except for thought proposer andstate evaluator for the Game of 24 evaluation, where the\\ntemperature is set to 0.7. Since OpenAI has randomness in outputs even with temperature 0, we have conducted 3 runs\\nand reported the average accuracy. Across ReAct, OpenAI parallel function calling, and LLMCompiler , we perform\\n3, 1, and 5-shot learning for HotpotQA, Movie Recommendation, and ParallelQA, respectively; the same examples\\nacross different methods were used to ensure a fair comparison. For the Game of 24, we use 2 in-context examples for\\nthe Planner. We use the same instruction prompts across different methods for a fair comparison, except for ReAct†\\nin Sec. 4.1 with additional ReAct-specific prompts. For WebShop experiment, we use gpt-4-0613 with 8k context\\nwindow and gpt-3.5-turbo model with 16k context window.\\nA.5 Analysis\\nA.5.1 Parallel Speedup Modeling\\nWhile LLMCompiler shows noticeable latency gain in various workloads, it is not achieving the N×latency speedup\\nfor N-way parallel workloads. This is mostly due to the overhead associated with LLMCompiler ’s Planner and final\\nanswering process that cannot be parallelized. In our Movie Recommendation experiment, LLMCompiler ’s Planner\\nand the answering process have an overhead of 1.88 and 1.62 seconds on average, respectively, whose combined\\noverhead already comprises more than half of LLMCompiler ’s overall latency in Tab 1. Another source of overhead\\nis the straggler effect among the parallel tasks when they need to join together. We observe the average latency of the\\nslowest search to be 1.13 seconds which is nearly 2×the average latency of all tasks, which is 0.61 seconds. Below,\\nwe provide an analytical latency modeling of ReAct, LLMCompiler , and LLMCompiler with streaming, and we\\nprovide an analysis of achievable latency speedup.\\nIn this section, our focus is on embarrassingly parallelizable workload (pattern Figure 3(a)), as this allows for a\\nclearer understanding of the impact of each component on potential latency gains. For the precise latency analysis, we\\nconsider three key components: the Planner, the Task Fetching Unit, and the Executor, in Figure 2. Assume that the\\nPlanner generates Ndifferent tasks to be done. We define Pias the Planner’s output corresponding to the i-th atomic\\ntask. Each Piis a blueprint for a specific atomic task, which we refer to as Ei. The execution of Eiinvolves a specific\\nfunction call using the appropriate tool.', start_char_idx=0, end_char_idx=4131, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8427049560467622), NodeWithScore(node=TextNode(id_='9c9b7155-30b9-40af-a071-7e2d557a538f', embedding=None, metadata={'page_label': '3', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='25035116-56a8-48a5-b983-aadf47c03175', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '3', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, hash='972de646988b3923513c1654c4d04fbb1ba1d0d7f9e0704b71321e4a20d285ae'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='5e7a4fe5-faa2-4e15-8e59-0624f4dd0728', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '3', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, hash='03e7fa2f226a5c5cb9888321770baaca148eeded3aae4b0077397e5e24be5cd4'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d2b35a35-3bce-4bb5-b8ea-0402731da59f', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='095e48e09d0ca6abfa79cadcd0c3803b9bdf59133c75377a71427cff1c84b489')}, text='A notable work is ReWOO [53] which employs a planner to separate the reasoning process from the execution and\\nobservation phases to decrease token usage and cost as compared to ReAct. Our approach is different from ReWOO\\nin multiple aspects. First, LLMCompiler allows parallel function calling which can reduce latency as well as cost.\\nSecond, LLMCompiler supports dynamic replanning which is important for problems whose execution flow cannot\\nbe determined statically in the beginning (Sec. 4.3).\\n2.3 Tool-Augmented LLMs\\nA notable work is Toolformer [43], which produces a custom LLM output to let the LLM decide what the inputs\\nfor calling the functions should be and where to insert the result. This approach has inspired various tool calling\\nframeworks [29, 44]. ReAct [58] proposed to have LLMs interact with external environments through reasoning and\\naction generation for improved performance. Gorilla [39] introduced a finetuned LLM designed for function calling,\\nand ToolLLM [41] and RestGPT [46] have extended LLMs to support real-world APIs. Moreover, OpenAI [35]\\nreleased their own function calling capabilities, allowing their LLMs to return formatted JSON for execution.\\n3', start_char_idx=3448, end_char_idx=4635, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8405651957303741)]\n",
      "{'5e7a4fe5-faa2-4e15-8e59-0624f4dd0728': {'page_label': '3', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, '46d166ba-ebc1-4a16-8aaa-fdc8a0770341': {'page_label': '18', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, '9c9b7155-30b9-40af-a071-7e2d557a538f': {'page_label': '3', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}}\n"
     ]
    }
   ],
   "source": [
    "resp = query_engine_nodes.query(\"Explain LLMCompiler and its usecases.\")\n",
    "\n",
    "print(resp.response)\n",
    "print(resp.source_nodes)\n",
    "print(resp.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node-0 has the score of 0.8525170579800175\n",
      "Node-1 has the score of 0.8427049560467622\n",
      "Node-2 has the score of 0.8405651957303741\n"
     ]
    }
   ],
   "source": [
    "for idx, node_with_score in enumerate(resp.source_nodes):\n",
    "    print(f'Node-{idx} has the score of {node_with_score.score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tool and ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tool = QueryEngineTool.from_defaults(\n",
    "    name='llmcompiler',\n",
    "    query_engine=query_engine_nodes,\n",
    "    description=(\n",
    "        \"Provides information about LLMCompiler and Parallel Function Calling.\"\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "react_agent = ReActAgent.from_tools(\n",
    "    [query_engine_tool],\n",
    "    max_function_calls=10,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Ask From Our Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: I need to use a tool to help me answer the question.\n",
      "Action: llmcompiler\n",
      "Action Input: {'input': 'Parallel Function Calling'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: LLMCompiler introduces a method for executing functions in parallel to efficiently manage multiple function calls. It consists of three main components: an LLM Planner for formulating execution plans, a Task Fetching Unit for dispatching function calling tasks, and an Executor for executing these tasks concurrently. By leveraging parallel execution, LLMCompiler optimizes the orchestration of function calls, resulting in significant improvements in latency speedup, cost savings, and accuracy compared to sequential methods like ReAct.\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I need to use a tool to help me answer the question.\n",
      "Action: llmcompiler\n",
      "Action Input: {'input': 'LLMCompilers'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: LLMCompiler is introduced as a tool that enables parallel function calling to efficiently orchestrate multiple function calls. It consists of three main components: an LLM Planner for formulating execution plans, a Task Fetching Unit for dispatching function calling tasks, and an Executor for executing these tasks in parallel. LLMCompiler aims to streamline parallel function calling by automatically generating optimized orchestrations for function calls, resulting in consistent latency speedup, cost savings, and accuracy improvements compared to existing methods like ReAct.\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: I can answer without using any more tools.\n",
      "Answer: Parallel Function Calling is a method of executing multiple functions concurrently to improve efficiency and performance. LLMCompilers help by automating the process of orchestrating these parallel function calls, leading to faster execution, cost savings, and improved accuracy compared to traditional sequential methods.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "react_agent_resp = react_agent.chat(\"What is Parallel Function Calling and How LLMCompilers can help?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE: Parallel Function Calling is a method of executing multiple functions concurrently to improve efficiency and performance. LLMCompilers help by automating the process of orchestrating these parallel function calls, leading to faster execution, cost savings, and improved accuracy compared to traditional sequential methods.\n",
      "OBJECT_KEYS: dict_keys(['response', 'sources', 'source_nodes'])\n",
      "SOURCE_NODES: [NodeWithScore(node=TextNode(id_='1ec932d3-31ba-4cba-ac98-1cc5b15a663e', embedding=None, metadata={'page_label': '1', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8473dcf1-e976-4c29-a45c-aa690fed5ca1', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, hash='9dac9404d07def90472d0270b947d6bd5dcb187d76d14b2e61df4cf98da4d726'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='0521f786-d0be-4e03-865b-029194269d18', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': '../files/papers/i.py', 'file_name': 'i.py', 'file_type': 'text/x-python', 'file_size': 0, 'creation_date': '2024-01-22', 'last_modified_date': '2024-01-22', 'last_accessed_date': '2024-01-22'}, hash='392472b1e230772808c521b7a991a07afb67e4b13bccaa33eb9bf9d004c25f06'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='e42a6f20-1554-45c1-ad7c-f5071ed50ea4', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='6ae7aa86bbe8c37deb0b96877cfe57be6e5b55ab96fae0e7c23a519612d38de4')}, text='An LLM Compiler for Parallel Function Calling\\nSehoon Kim∗1Suhong Moon∗1Ryan Tabrizi1Nicholas Lee1\\nMichael W. Mahoney1,2,3Kurt Keutzer1Amir Gholami1,2\\n1UC Berkeley2ICSI3LBNL\\n{sehoonkim, suhong.moon, rtabrizi, nicholas lee, mahoneymw, keutzer, amirgh }@berkeley.edu\\nAbstract\\nRecent language models have shown remarkable results on various complex reasoning benchmarks. The rea-\\nsoning capabilities of LLMs enable them to execute external function calls to overcome their inherent limitations,\\nsuch as knowledge cutoffs, poor arithmetic skills, or lack of access to private data. This development has allowed\\nLLMs to select and coordinate multiple functions based on the context to tackle more complex problems. However,\\ncurrent methods for multiple function calling often require sequential reasoning and acting for each function which\\ncan result in high latency, cost, and sometimes inaccurate behavior. To address this, we introduce LLMCompiler ,\\nwhich executes functions in parallel to efficiently orchestrate multiple function calling. Drawing from the principles\\nof classical compilers, LLMCompiler streamlines parallel function calling with three components: (i) an LLM Plan-\\nner, formulating execution plans; (ii) a Task Fetching Unit, dispatching function calling tasks; and (iii) an Executor,\\nexecuting these tasks in parallel. LLMCompiler automatically generates an optimized orchestration for the function\\ncalls and can be used with both open-source and closed-source models. We have benchmarked LLMCompiler on\\na range of tasks with different patterns of function calling. We observe consistent latency speedup of up to 3.7×,\\ncost savings of up to 6.7×, and accuracy improvement of up to ∼9%compared to ReAct. Our code is available at\\nhttps://github.com/SqueezeAILab/LLMCompiler .\\n1 Introduction\\nRecent advances in the reasoning capability of Large Language Models (LLMs) have expanded the applicability of\\nLLMs beyond content generation to solving complex problems [51, 23, 57, 5, 50, 63, 8, 55, 12]; and recent works have\\nshown how this reasoning capability could be helpful in improving accuracy for solving complex and logical tasks.\\nThe reasoning capability has also allowed function (i.e., tool) calling capability where LLMs can invoke provided\\nfunctions and use the function outputs to help complete their tasks. These functions range from a simple calculator\\nthat can invoke arithmetic operations to more complex LLM-based functions.\\nThe ability of LLMs to integrate various tools and function calls could enable a fundamental shift in how we\\ndevelop LLM-based software. However, this brings up an important challenge: what is the most effective approach\\nto incorporate multiple function calls? A notable approach has been introduced in ReAct [58], where the LLM calls\\na function, analyzes the outcomes, and then reasons about the next action, which involves a subsequent function call.\\nFor a simple example illustrated in Figure 1 (Left), where the LLM is asked if Scott Derrickson and Ed Wood have the\\nsame nationality, ReAct initially analyzes the query and decides to use a search tool to search for Scott Derrickson.\\nThe result of this search (i.e., observation) is then concatenated back to the original prompt for the LLM to reason\\nabout the next action, which invokes another search tool to gather information about Ed Wood.\\nReAct has been a pioneering work in enabling function calling, and it has been integrated into several frame-\\nworks [26, 31]. However, scaling this approach for more complex applications requires considerable optimizations.\\nThis is due to the sequential nature of ReAct, where it executes function calls and reasons about their observations\\none after the other. This approach, along with the agent systems that extend ReAct [20, 57, 41, 42, 47], may lead\\nto inefficiencies in latency and cost due to the sequential function calling and repetitive LLM invocations for each\\nreasoning and action step. Furthermore, while dynamic reasoning about the observations has benefits in certain cases,\\n*Equal contribution\\n1arXiv:2312.04511v2  [cs.CL]  6 Feb 2024', start_char_idx=0, end_char_idx=4101, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8288172844945669), NodeWithScore(node=TextNode(id_='6e0d394c-782e-4660-8398-82f464d5e9d0', embedding=None, metadata={'page_label': '20', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='55c9f8cd-03a2-4666-91d2-b2f32a63cceb', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '20', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, hash='49e8964177aca67555da7a230f0d3e46a86a7009541714bed9517e8f064c798f'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c77f6ca4-bc23-4958-a76b-392c8dd5420e', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '19', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, hash='ef38a9d8a85fa2d57d71683026c585f87180699d35650369b00972e08b7375ee'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='720d8142-0cc7-4734-bc9a-e261cc22cafd', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='de93bf046c6fa64087baf99b1e27fd322ffc219a78219572f0e4f002722d339f')}, text='2 3 4 5\\nNumber of Parallelizable T asks01020304050Latency (s)\\nLatency vs. # Parallelizable T asks\\nReAct\\nLLMCompiler (Ours)Figure A.5: Latency on the ParallelQA benchmark grouped by the number of maximum parallelizable tasks.\\nA.5.2 Latency versus Number of Parallelizable Tasks\\nIn Figure A.5, we also report a more detailed latency breakdown on ParallelQA where we show the end-to-end latency\\nas a function of the number of parallel tasks. This is often referred to as weak-scaling in high-performance computing,\\nwhere the ideal behavior is to have a constant latency as the number of tasks is increased. We can see that ReAct’s\\nlatency increases proportionally to the number of tasks which is expected as it executes the tasks sequentially. In\\ncontrast, the latency of LLMCompiler increases at a much smaller rate as it can perform multiple function calls\\nin parallel when possible. The reason the end-to-end latency increases slightly with LLMCompiler is due to the\\noverhead of the Planner, which needs to generate plans initially, and which cannot be parallelized. We provide a\\nfurther analysis of this in Appendix A.5.1.\\nA.6 User-Supplied Examples for LLMCompiler Configuration\\nLLMCompiler provides a simple interface that allows for tailoring the framework to different use cases by providing\\ntool definitions as well as optional in-context examples for the Planner. Below, we provide the Planner example\\nprompts that are used to set up the framework for the Movie Recommendation and Game of 24 benchmarks with only\\na few lines of prompts.\\nA.6.1 Movie Recommendation Example Prompts\\nQuestion: Find a movie similar to Mission Impossible, The Silence of the\\nLambs, American Beauty, Star Wars Episode IV - A New Hope\\nOptions:\\nAustin Powers International Man of Mystery\\nAlesha Popvich and Tugarin the Dragon\\nIn Cold Blood\\nRosetta\\n1. search(\"Mission Impossible\")\\n2. search(\"The Silence of the Lambs\")\\n3. search(\"American Beauty\")\\n4. search(\"Star Wars Episode IV - A New Hope\")\\n5. search(\"Austin Powers International Man of Mystery\")\\n6. search(\"Alesha Popvich and Tugarin the Dragon\")\\n7. search(\"In Cold Blood\")\\n8. search(\"Rosetta\")\\nThought: I can answer the question now.\\n9. join()\\n###\\n20', start_char_idx=0, end_char_idx=2187, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7933399276438737), NodeWithScore(node=TextNode(id_='413f21e4-b8db-49e1-a2d6-763ca8995fe1', embedding=None, metadata={'page_label': '5', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='cb70b1cc-83d9-456a-a219-9b72a93a42a9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '5', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, hash='da011dea72a0aada306d772de9c5b5918b584e7aaa9d774bda1cea8f2eaae2c0'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='d2b35a35-3bce-4bb5-b8ea-0402731da59f', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '4', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, hash='ac423c51bd23b5cc27d6668be80d507914eec678ee6835dd173c7eaeabd26de5'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='105f276d-9615-4d0c-882e-e5b68cf7897a', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='cd77030150d50fb45e6635a032f0e23cee5e9dbb1bb60e7860d52a2698fa1970')}, text=\"search\\nmathsearch\\nmath(c) Which has higher total healthcare expenses, Florida \\nor New York, considering both public and private sectors?\\nsearch\\nmathsearch search\\nmathsearch\\nmath(b) If Stanford and UCLA were to merge, would they \\nhave more Nobel laureates than UC Berkeley?\\nsearchAnalyzer \\nAgent\\noutput\\noutput output(a) Analyze Apple and Microsoft's latest 10 -K \\nform and compare their sales forecast.\\nAnalyzer \\nAgentFigure 3: Examples of questions with different function calling patterns and their dependency graphs. HotpotQA\\nand Movie Recommendation datasets exhibit pattern (a), and ParallelQA dataset exhibits patterns (b) and (c), among\\nother patterns. In (a), we need to analyze each company’s latest 10-K. In (b), we need three searches for each school,\\nfollowed by one addition and one comparison operation. In (c), we need to search for each state’s annual healthcare\\nspending in each sector, sum each state’s spending, and then perform a comparison.\\nis to replace variables with the actual outputs from preceding tasks, which were initially set as placeholders by the\\nPlanner. For the example in Figure 2, the variable $1and$2in Task $3would be replaced with the actual market cap\\nof Microsoft and Apple after the search tasks are done. This can be implemented with a simple fetching and queuing\\nmechanism without a dedicated LLM.\\n3.3 Executor\\nThe Executor asynchronously executes tasks fetched from the Task Fetching Unit. As the Task Fetching Unit guaran-\\ntees all the tasks dispatched to the Executor are independent, it can simply execute them concurrently. The Executor is\\nequipped with user-provided tools, and it delegates the task to the associated tool. These tools can be simple functions\\nlike a calculator, Wikipedia search, or API calls, or they can even be LLM agents that are tailored for a specific task. As\\ndepicted in the Executor block of Figure 2, each task has dedicated memory to store its intermediate outcomes, similar\\nto what typical sequential frameworks do when aggregating observations as a single prompt [58]. Upon completion of\\nthe task, the final results are forwarded as input to the tasks dependent on them.\\n3.4 Dynamic Replanning\\nIn various applications, the execution graph may need to adapt based on intermediate results that are a priori unknown.\\nA similar analogy in programming is branching, where the path of execution is determined only during runtime,\\ndepending on which branch conditions are satisfied. Such dynamic execution patterns can also appear with LLM\\nfunction calling. For simple branching (e.g., if-else statements) one could statically compile the execution flow and\\nchoose the right dynamically based on the intermediate results. However, for more complex branching it may be\\nbetter to do a recompilation or replanning based on the intermediate results. In replanning, the Executor sends the\\nintermediate results back to our LLM Planner. Based on that, the Planner produces a new set of tasks with their\\nassociated dependencies and dispatches them to the Task Fetching Unit and then the Executor. This process is repeated\\nuntil the final result is achieved. We show an example use case of this in Sec. 4.3 for solving the Game of 24 using the\\nTree-of-Thoughts approach.\\n4 Results\\nIn this section, we evaluate LLMCompiler using a variety of models and problem types. We use both the propri-\\netary GPT models and the open-source LLaMA-2 model, with the latter demonstrating LLMCompiler ’s capability\\nin enabling parallel function calling in open-source models. Furthermore, there are various types of parallel function\\ncalling patterns that can be addressed with LLMs. This ranges from embarrassingly parallel patterns, where all tasks\\ncan be executed in parallel without any dependencies between them, to more complex dependency patterns, as illus-\\ntrated in Figure 3. Significantly, we also assess LLMCompiler on the Game of 24 benchmark involving dynamic\\nreplanning based on intermediate results, highlighting its adaptability to dynamic dependency graphs. Finally, we\\napply LLMCompiler to the WebShop benchmark to showcase its potential in decision-making tasks. In this section,\\nwe start presenting results for simple execution patterns, and then we move to more complex ones.\\n5\", start_char_idx=0, end_char_idx=4246, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7913017065129665), NodeWithScore(node=TextNode(id_='5e7a4fe5-faa2-4e15-8e59-0624f4dd0728', embedding=None, metadata={'page_label': '3', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='25035116-56a8-48a5-b983-aadf47c03175', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '3', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, hash='972de646988b3923513c1654c4d04fbb1ba1d0d7f9e0704b71321e4a20d285ae'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e42a6f20-1554-45c1-ad7c-f5071ed50ea4', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '2', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, hash='7c509b808f481df38b8b04478e63d7c6f132f5538326081dbe1c7ad325b54843'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='9c9b7155-30b9-40af-a071-7e2d557a538f', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='2c212ec17504a612ac3a21c424a157c3a73ecb5142acbf1dcdb3a991cdf6b501')}, text='• We evaluate LLMCompiler ’s capability in dynamic replanning, which is achieved through a feedback loop from\\nthe Executor back to our LLM Planner. For the Game of 24 [57], which requires repeated replanning based on the\\nintermediate results, LLMCompiler demonstrates a 2 ×speedup compared to Tree-of-Thoughts (Sec. 4.3).\\n• We showcase that LLMCompiler can explore the interactive decision-making environment effectively and effi-\\nciently. On WebShop, LLMCompiler achieves up to 101.7×speedup and 25.7% improved success rate compared\\nto the baselines. (Sec. 4.4)\\n2 Related Work\\n2.1 Latency Optimization in LLMs\\nVarious studies have focused on optimizing model design [21, 11, 30, 9, 24, 10, 22, 6, 28] and systems [25, 59, 1, 2]\\nfor efficient LLM inference. Optimizations at the application level, however, are less explored. This is critical from a\\npractical point of view for situations involving black-box LLM models and services where modifications to the models\\nand the underlying inference pipeline are highly restricted.\\nSkeleton-of-Thought [34] recently proposed to reduce latency through application-level parallel decoding. This\\nmethod involves a two-step process of an initial skeleton generation phase, followed by parallel execution of skeleton\\nitems. However, it is primarily designed for embarrassingly parallel workloads and does not support problems that\\nhave inherently interdependent tasks, as it assumes no dependencies between skeleton tasks. This limits its applica-\\nbility in complex scenarios such as coding [7, 33, 14, 3] or math [16, 15] problems, as also stated in the paper [34].\\nLLMCompiler addresses this by translating an input query into a series of tasks with inter-dependencies, thereby\\nexpanding the spectrum of problems it can handle.\\nConcurrently to our work, OpenAI has recently introduced a parallel function calling feature in their 1106 re-\\nlease, enhancing user query processing through the simultaneous generation of multiple function calls [36]. Despite\\nits potential for reducing LLM execution time, this feature has certain limitations as it is exclusively available for\\nOpenAI’s proprietary models. However, there is a growing demand for using open-source models driven by the in-\\ncreasing number of open-source LLMs as well as parameter-efficient training techniques [27, 18, 17] for finetuning\\nand customization. LLMCompiler enables efficient parallel function calling for open-source models, and also, as we\\nwill show later in Sec. 4, it can potentially achieve better latency and cost.\\n2.2 Plan and Solve Strategy\\nSeveral studies [52, 38, 40, 63, 13] have explored prompting methods of breaking down complex queries into various\\nlevels of detail to solve them, thereby improving LLM’s performance in reasoning tasks. Specifically, Decomposed\\nPrompting [20] tackles complex tasks by decomposing them into simpler sub-tasks, each optimized through LLMs\\nwith dedicated prompts. Step-Back Prompting [61] enables LLMs to abstract high-level concepts from details to\\nenhance reasoning abilities across various tasks. Plan-and-Solve Prompting [49] segments multi-step reasoning tasks\\ninto subtasks to minimize errors and improve task accuracy without manual prompting. However, these methods\\nprimarily focus on improving the accuracy of reasoning benchmarks. In contrast, LLMCompiler uses a planner to\\nidentify parallelizable patterns within queries, aiming to reduce latency while maintaining accuracy.\\nA notable work is ReWOO [53] which employs a planner to separate the reasoning process from the execution and\\nobservation phases to decrease token usage and cost as compared to ReAct. Our approach is different from ReWOO\\nin multiple aspects. First, LLMCompiler allows parallel function calling which can reduce latency as well as cost.\\nSecond, LLMCompiler supports dynamic replanning which is important for problems whose execution flow cannot\\nbe determined statically in the beginning (Sec. 4.3).\\n2.3 Tool-Augmented LLMs\\nA notable work is Toolformer [43], which produces a custom LLM output to let the LLM decide what the inputs\\nfor calling the functions should be and where to insert the result. This approach has inspired various tool calling\\nframeworks [29, 44]. ReAct [58] proposed to have LLMs interact with external environments through reasoning and\\naction generation for improved performance.', start_char_idx=0, end_char_idx=4349, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8318630089198781), NodeWithScore(node=TextNode(id_='9c9b7155-30b9-40af-a071-7e2d557a538f', embedding=None, metadata={'page_label': '3', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='25035116-56a8-48a5-b983-aadf47c03175', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '3', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, hash='972de646988b3923513c1654c4d04fbb1ba1d0d7f9e0704b71321e4a20d285ae'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='5e7a4fe5-faa2-4e15-8e59-0624f4dd0728', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '3', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, hash='03e7fa2f226a5c5cb9888321770baaca148eeded3aae4b0077397e5e24be5cd4'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d2b35a35-3bce-4bb5-b8ea-0402731da59f', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='095e48e09d0ca6abfa79cadcd0c3803b9bdf59133c75377a71427cff1c84b489')}, text='A notable work is ReWOO [53] which employs a planner to separate the reasoning process from the execution and\\nobservation phases to decrease token usage and cost as compared to ReAct. Our approach is different from ReWOO\\nin multiple aspects. First, LLMCompiler allows parallel function calling which can reduce latency as well as cost.\\nSecond, LLMCompiler supports dynamic replanning which is important for problems whose execution flow cannot\\nbe determined statically in the beginning (Sec. 4.3).\\n2.3 Tool-Augmented LLMs\\nA notable work is Toolformer [43], which produces a custom LLM output to let the LLM decide what the inputs\\nfor calling the functions should be and where to insert the result. This approach has inspired various tool calling\\nframeworks [29, 44]. ReAct [58] proposed to have LLMs interact with external environments through reasoning and\\naction generation for improved performance. Gorilla [39] introduced a finetuned LLM designed for function calling,\\nand ToolLLM [41] and RestGPT [46] have extended LLMs to support real-world APIs. Moreover, OpenAI [35]\\nreleased their own function calling capabilities, allowing their LLMs to return formatted JSON for execution.\\n3', start_char_idx=3448, end_char_idx=4635, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8298798576027977), NodeWithScore(node=TextNode(id_='1ec932d3-31ba-4cba-ac98-1cc5b15a663e', embedding=None, metadata={'page_label': '1', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8473dcf1-e976-4c29-a45c-aa690fed5ca1', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'llm_compiler_2312.04511.pdf', 'file_path': '../files/papers/llm_compiler_2312.04511.pdf', 'file_type': 'application/pdf', 'file_size': 885090, 'creation_date': '2024-02-18', 'last_modified_date': '2024-02-07', 'last_accessed_date': '2024-02-18'}, hash='9dac9404d07def90472d0270b947d6bd5dcb187d76d14b2e61df4cf98da4d726'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='0521f786-d0be-4e03-865b-029194269d18', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': '../files/papers/i.py', 'file_name': 'i.py', 'file_type': 'text/x-python', 'file_size': 0, 'creation_date': '2024-01-22', 'last_modified_date': '2024-01-22', 'last_accessed_date': '2024-01-22'}, hash='392472b1e230772808c521b7a991a07afb67e4b13bccaa33eb9bf9d004c25f06'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='e42a6f20-1554-45c1-ad7c-f5071ed50ea4', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='6ae7aa86bbe8c37deb0b96877cfe57be6e5b55ab96fae0e7c23a519612d38de4')}, text='An LLM Compiler for Parallel Function Calling\\nSehoon Kim∗1Suhong Moon∗1Ryan Tabrizi1Nicholas Lee1\\nMichael W. Mahoney1,2,3Kurt Keutzer1Amir Gholami1,2\\n1UC Berkeley2ICSI3LBNL\\n{sehoonkim, suhong.moon, rtabrizi, nicholas lee, mahoneymw, keutzer, amirgh }@berkeley.edu\\nAbstract\\nRecent language models have shown remarkable results on various complex reasoning benchmarks. The rea-\\nsoning capabilities of LLMs enable them to execute external function calls to overcome their inherent limitations,\\nsuch as knowledge cutoffs, poor arithmetic skills, or lack of access to private data. This development has allowed\\nLLMs to select and coordinate multiple functions based on the context to tackle more complex problems. However,\\ncurrent methods for multiple function calling often require sequential reasoning and acting for each function which\\ncan result in high latency, cost, and sometimes inaccurate behavior. To address this, we introduce LLMCompiler ,\\nwhich executes functions in parallel to efficiently orchestrate multiple function calling. Drawing from the principles\\nof classical compilers, LLMCompiler streamlines parallel function calling with three components: (i) an LLM Plan-\\nner, formulating execution plans; (ii) a Task Fetching Unit, dispatching function calling tasks; and (iii) an Executor,\\nexecuting these tasks in parallel. LLMCompiler automatically generates an optimized orchestration for the function\\ncalls and can be used with both open-source and closed-source models. We have benchmarked LLMCompiler on\\na range of tasks with different patterns of function calling. We observe consistent latency speedup of up to 3.7×,\\ncost savings of up to 6.7×, and accuracy improvement of up to ∼9%compared to ReAct. Our code is available at\\nhttps://github.com/SqueezeAILab/LLMCompiler .\\n1 Introduction\\nRecent advances in the reasoning capability of Large Language Models (LLMs) have expanded the applicability of\\nLLMs beyond content generation to solving complex problems [51, 23, 57, 5, 50, 63, 8, 55, 12]; and recent works have\\nshown how this reasoning capability could be helpful in improving accuracy for solving complex and logical tasks.\\nThe reasoning capability has also allowed function (i.e., tool) calling capability where LLMs can invoke provided\\nfunctions and use the function outputs to help complete their tasks. These functions range from a simple calculator\\nthat can invoke arithmetic operations to more complex LLM-based functions.\\nThe ability of LLMs to integrate various tools and function calls could enable a fundamental shift in how we\\ndevelop LLM-based software. However, this brings up an important challenge: what is the most effective approach\\nto incorporate multiple function calls? A notable approach has been introduced in ReAct [58], where the LLM calls\\na function, analyzes the outcomes, and then reasons about the next action, which involves a subsequent function call.\\nFor a simple example illustrated in Figure 1 (Left), where the LLM is asked if Scott Derrickson and Ed Wood have the\\nsame nationality, ReAct initially analyzes the query and decides to use a search tool to search for Scott Derrickson.\\nThe result of this search (i.e., observation) is then concatenated back to the original prompt for the LLM to reason\\nabout the next action, which invokes another search tool to gather information about Ed Wood.\\nReAct has been a pioneering work in enabling function calling, and it has been integrated into several frame-\\nworks [26, 31]. However, scaling this approach for more complex applications requires considerable optimizations.\\nThis is due to the sequential nature of ReAct, where it executes function calls and reasons about their observations\\none after the other. This approach, along with the agent systems that extend ReAct [20, 57, 41, 42, 47], may lead\\nto inefficiencies in latency and cost due to the sequential function calling and repetitive LLM invocations for each\\nreasoning and action step. Furthermore, while dynamic reasoning about the observations has benefits in certain cases,\\n*Equal contribution\\n1arXiv:2312.04511v2  [cs.CL]  6 Feb 2024', start_char_idx=0, end_char_idx=4101, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8226568779788507)]\n"
     ]
    }
   ],
   "source": [
    "print(f'RESPONSE: {react_agent_resp.response}')\n",
    "print(f'OBJECT_KEYS: {react_agent_resp.__dict__.keys()}')\n",
    "print(f'SOURCE_NODES: {react_agent_resp.source_nodes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len SOURCE_NODES: 6\n"
     ]
    }
   ],
   "source": [
    "print(f'len SOURCE_NODES: {len(react_agent_resp.source_nodes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMemoryBuffer(token_limit=3072, tokenizer_fn=functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all'), chat_store=SimpleChatStore(store={'chat_history': [ChatMessage(role=<MessageRole.USER: 'user'>, content='What is Parallel Function Calling and How LLMCompilers can help?', additional_kwargs={}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Parallel Function Calling is a method of executing multiple functions concurrently to improve efficiency and performance. LLMCompilers help by automating the process of orchestrating these parallel function calls, leading to faster execution, cost savings, and improved accuracy compared to traditional sequential methods.', additional_kwargs={})]}), chat_store_key='chat_history')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "react_agent.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content='What is Parallel Function Calling and How LLMCompilers can help?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Parallel Function Calling is a method of executing multiple functions concurrently to improve efficiency and performance. LLMCompilers help by automating the process of orchestrating these parallel function calls, leading to faster execution, cost savings, and improved accuracy compared to traditional sequential methods.', additional_kwargs={})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "react_agent.memory.chat_store.store['chat_history']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
